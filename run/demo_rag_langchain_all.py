import os
import gradio as gr
from dotenv import load_dotenv
from tianji.knowledges.langchain_onlinellm.models import SiliconFlowEmbeddings, SiliconFlowLLM
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from tianji import TIANJI_PATH
import argparse
from huggingface_hub import snapshot_download
import requests
import loguru
load_dotenv()

parser = argparse.ArgumentParser(description='Launch Gradio application')
parser.add_argument('--listen', action='store_true', help='Specify to listen on 0.0.0.0')
parser.add_argument('--port', type=int, default=None, help='The port the server should listen on')
parser.add_argument('--root_path', type=str, default=None, help='The root path of the server')
parser.add_argument('--force', action='store_true', help='Force recreate the database')
parser.add_argument('--chunk_size', type=int, default=896, help='Chunk size for text splitting')
args = parser.parse_args()

# å¼€å§‹å‰æ£€æŸ¥åŠŸèƒ½æ˜¯å¦æ­£å¸¸
try:
    llm = SiliconFlowLLM()
    test_response = llm._call("ä½ å¥½")
    loguru.logger.info("SiliconFlowèŠå¤©åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
except Exception as e:
    loguru.logger.error("SiliconFlowèŠå¤©åŠŸèƒ½æµ‹è¯•å¤±è´¥: {}", str(e))
    raise e
try:
    embeddings = SiliconFlowEmbeddings()
    test_text = "æµ‹è¯•æ–‡æœ¬"
    test_embedding = embeddings.embed_query(test_text)
    if len(test_embedding) > 0:
        loguru.logger.info("SiliconFlowåµŒå…¥åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
    else:
        raise ValueError("åµŒå…¥å‘é‡é•¿åº¦ä¸º0")
except Exception as e:
    loguru.logger.error("SiliconFlowåµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥: {}", str(e))
    raise e

# æ­£å¼ä»£ç 
def check_internet_connection(url='http://www.google.com/', timeout=5):
    try:
        _ = requests.head(url, timeout=timeout)
        return True
    except requests.ConnectionError:
        return False
    
destination_folder = os.path.join(TIANJI_PATH, "temp", "tianji-chinese")
if not os.path.exists(destination_folder):
    if not check_internet_connection():
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    for _ in range(5):
        try:
            snapshot_download(
                repo_id="sanbu/tianji-chinese",
                local_dir=destination_folder,
                repo_type="dataset",
                local_dir_use_symlinks=False,
                endpoint=os.environ.get('HF_ENDPOINT', None),
            )
            break
        except Exception as e:
            loguru.logger.error("Download failed, retrying... Error message: {}", str(e))
    else:
        loguru.logger.error("Download failed, maximum retry count reached.")


def create_vectordb(
    data_path: str,
    persist_directory: str,
    embedding_func,
    chunk_size: int,
    force: bool = False,
):
    if os.path.exists(persist_directory) and not force:
        return Chroma(
            persist_directory=persist_directory, embedding_function=embedding_func
        )
    if force and os.path.exists(persist_directory):
        if os.path.isdir(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        else:
            os.remove(persist_directory)
    loader = DirectoryLoader(data_path, glob="*.txt", loader_cls=TextLoader)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(loader.load())
    if len(split_docs) == 0:
        loguru.logger.error("Invalid knowledge data, processing data results in empty, check if data download failed, can be downloaded manually")
        raise gr.Error("Invalid knowledge data, processing data results in empty, check if data download failed, can be downloaded manually")
    try:
        vector_db = Chroma.from_documents(
            documents=split_docs,
            embedding=embedding_func,
            persist_directory=persist_directory,
        )
    except Exception as e:
        loguru.logger.error("åˆ›å»ºæ•°æ®åº“å¤±è´¥: {}", str(e))
        raise e
    return vector_db


def initialize_chain(chunk_size: int, persist_directory: str, data_path: str, force=False):
    loguru.logger.info("åˆå§‹åŒ–æ•°æ®åº“å¼€å§‹ï¼Œå½“å‰æ•°æ®è·¯å¾„ä¸ºï¼š{}", data_path)
    vectordb = create_vectordb(data_path, persist_directory, embeddings, chunk_size, force)
    retriever = vectordb.as_retriever()
    prompt = hub.pull("rlm/rag-prompt")
    prompt.messages[
        0
    ].prompt.template = """
    æ‚¨æ˜¯ä¸€åç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰é«˜åº¦ç›¸å…³ä¸Šä¸‹æ–‡ ä½ å°±è‡ªç”±å›ç­”ã€‚\
    æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œç»“åˆæˆ‘çš„é—®é¢˜,ç›´æ¥ç»™å‡ºæœ€åçš„å›ç­”ï¼Œè¦åªç´§æ‰£é—®é¢˜å›´ç»•ç€å›ç­”ï¼Œå°½é‡æ ¹æ®æ¶‰åŠå‡ ä¸ªå…³é”®ç‚¹ç”¨å®Œæ•´éå¸¸è¯¦ç»†çš„å‡ æ®µè¯å›å¤ã€‚ã€‚\
    \né—®é¢˜ï¼š{question} \nä¸Šä¸‹æ–‡ï¼š{context} \nå›ç­”ï¼š
    """
    loguru.logger.info("åˆå§‹åŒ–æ•°æ®åº“ç»“æŸ")
    return (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def handle_question(chain, question: str, chat_history):
    if not question:
        return "", chat_history
    try:
        result = chain.invoke(question)
        chat_history.append((question, result))
        return "", chat_history
    except Exception as e:
        loguru.logger.error("å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {}", str(e))
        return str(e), chat_history


# Define scenarios
scenarios = {
    "æ•¬é…’ç¤¼ä»ªæ–‡åŒ–": "1-etiquette",
    "è¯·å®¢ç¤¼ä»ªæ–‡åŒ–": "2-hospitality",
    "é€ç¤¼ç¤¼ä»ªæ–‡åŒ–": "3-gifting",
    "å¦‚ä½•è¯´å¯¹è¯": "5-communication",
    "åŒ–è§£å°´å°¬åœºåˆ": "6-awkwardness",
    "çŸ›ç›¾&å†²çªåº”å¯¹": "7-conflict",
}

# Initialize chains for all scenarios
chains = {}
for scenario_name, scenario_folder in scenarios.items():
    data_path = os.path.join(
        TIANJI_PATH, "temp", "tianji-chinese", "RAG", scenario_folder
    )
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data path does not exist: {data_path}")

    persist_directory = os.path.join(TIANJI_PATH, "temp", f"chromadb_{scenario_folder}")
    chains[scenario_name] = initialize_chain(args.chunk_size, persist_directory, data_path, args.force)

# Create Gradio interface
TITLE = """
# Tianji äººæƒ…ä¸–æ•…å¤§æ¨¡å‹ç³»ç»Ÿå®Œæ•´ç‰ˆ(åŸºäºçŸ¥è¯†åº“å®ç°) æ¬¢è¿starï¼\n
## ğŸ’«å¼€æºé¡¹ç›®åœ°å€ï¼šhttps://github.com/SocialAI-tianji/Tianji
## ä½¿ç”¨æ–¹æ³•ï¼šé€‰æ‹©ä½ æƒ³æé—®çš„åœºæ™¯ï¼Œè¾“å…¥æç¤ºï¼Œæˆ–ç‚¹å‡»Exampleè‡ªåŠ¨å¡«å……
## å¦‚æœè§‰å¾—å›ç­”ä¸æ»¡æ„,å¯è¡¥å……æ›´å¤šä¿¡æ¯é‡å¤æé—®ã€‚
### æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯æ„å»ºä¸€ä¸ªä»æ•°æ®æ”¶é›†å¼€å§‹çš„å¤§æ¨¡å‹å…¨æ ˆå‚ç›´é¢†åŸŸå¼€æºå®è·µ.
"""


def get_examples_for_scenario(scenario):
    # Define examples for each scenario
    examples_dict = {
        "æ•¬é…’ç¤¼ä»ªæ–‡åŒ–": [
            "å–é…’åº§ä½æ€ä¹ˆæ’",
            "å–é…’çš„å…ˆåé¡ºåºæµç¨‹æ˜¯ä»€ä¹ˆ",
            "å–é…’éœ€è¦æ³¨æ„ä»€ä¹ˆ",
            "æ¨èçš„æ•¬é…’è¯æ€ä¹ˆè¯´",
            "å®´ä¼šæ€ä¹ˆç‚¹èœ",
            "å–é…’å®¹æ˜“é†‰æ€ä¹ˆåŠ",
            "å–é…’çš„è§„çŸ©æ˜¯ä»€ä¹ˆ",
        ],
        "è¯·å®¢ç¤¼ä»ªæ–‡åŒ–": ["è¯·å®¢æœ‰é‚£äº›è§„çŸ©", "å¦‚ä½•é€‰æ‹©åˆé€‚çš„é¤å…", "æ€ä¹ˆè¯·åˆ«äººåƒé¥­"],
        "é€ç¤¼ç¤¼ä»ªæ–‡åŒ–": ["é€ä»€ä¹ˆç¤¼ç‰©ç»™é•¿è¾ˆå¥½", "æ€ä¹ˆé€ç¤¼", "å›ç¤¼çš„ç¤¼èŠ‚æ˜¯ä»€ä¹ˆ"],
        "å¦‚ä½•è¯´å¯¹è¯": [
            "æ€ä¹ˆå’Œå¯¼å¸ˆæ²Ÿé€š",
            "æ€ä¹ˆæé«˜æƒ…å•†",
            "å¦‚ä½•è¯»æ‡‚æ½œå°è¯",
            "æ€ä¹ˆå®‰æ…°åˆ«äºº",
            "æ€ä¹ˆå’Œå­©å­æ²Ÿé€š",
            "å¦‚ä½•ä¸ç”·ç”ŸèŠå¤©",
            "å¦‚ä½•ä¸å¥³ç”ŸèŠå¤©",
            "èŒåœºé«˜æƒ…å•†å›åº”æŠ€å·§",
        ],
        "åŒ–è§£å°´å°¬åœºåˆ": ["æ€ä¹ˆå›åº”èµç¾", "æ€ä¹ˆæ‹’ç»å€Ÿé’±", "å¦‚ä½•é«˜æ•ˆæ²Ÿé€š", "æ€ä¹ˆå’Œå¯¹è±¡æ²Ÿé€š", "èŠå¤©æŠ€å·§", "æ€ä¹ˆæ‹’ç»åˆ«äºº", "èŒåœºæ€ä¹ˆæ²Ÿé€š"],
        "çŸ›ç›¾&å†²çªåº”å¯¹": [
            "æ€ä¹ˆæ§åˆ¶æƒ…ç»ª",
            "æ€ä¹ˆå‘åˆ«äººé“æ­‰",
            "å’Œåˆ«äººåµæ¶äº†æ€ä¹ˆåŠ",
            "å¦‚ä½•åŒ–è§£å°´å°¬",
            "å­©å­æœ‰æƒ…ç»ªæ€ä¹ˆåŠ",
            "å¤«å¦»åµæ¶æ€ä¹ˆåŠ",
            "æƒ…ä¾£å†·æˆ˜æ€ä¹ˆåŠ",
        ],
    }
    return examples_dict.get(scenario, [])


with gr.Blocks() as demo:
    gr.Markdown(TITLE)

    init_status = gr.Textbox(label="åˆå§‹åŒ–çŠ¶æ€", value="æ•°æ®åº“å·²åˆå§‹åŒ–", interactive=False)

    with gr.Tabs() as tabs:
        for scenario_name in scenarios.keys():
            with gr.Tab(scenario_name):
                chatbot = gr.Chatbot(height=450, show_copy_button=True)
                msg = gr.Textbox(label="è¾“å…¥ä½ çš„ç–‘é—®")

                examples = gr.Examples(
                    label="å¿«é€Ÿç¤ºä¾‹",
                    examples=get_examples_for_scenario(scenario_name),
                    inputs=[msg],
                )

                with gr.Row():
                    chat_button = gr.Button("èŠå¤©")
                    clear_button = gr.ClearButton(components=[chatbot], value="æ¸…é™¤èŠå¤©è®°å½•")

                # Define a function to invoke the chain for the current scenario
                def invoke_chain(question, chat_history, scenario=scenario_name):
                    loguru.logger.info(question)
                    return handle_question(chains[scenario], question, chat_history)

                chat_button.click(
                    invoke_chain,
                    inputs=[msg, chatbot],
                    outputs=[msg, chatbot],
                )


if __name__ == "__main__":
    server_name = '0.0.0.0' if args.listen else None
    server_port = args.port
    demo.launch(server_name=server_name, server_port=server_port, root_path=args.root_path)
